{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd5f0c8",
   "metadata": {},
   "source": [
    "# Immovlan Property Scraper\n",
    "\n",
    "This notebook collects property listings from [Immovlan.be](https://immovlan.be) and extracts detailed information for each property. It works in two main stages:\n",
    "\n",
    "1. **Collect all property links** by paginating through search results and following the “next” button.\n",
    "2. **Scrape details** from each property page and save them to a CSV file.\n",
    "\n",
    "The scraper uses a dynamic price‑based batching technique to overcome the site’s 50‑page limit: after each batch, it takes the highest price from that batch and uses it as the new minimum price for the next batch, effectively fetching all listings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3c16d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We’ll need the following modules:\n",
    "- `requests` – to download web pages.\n",
    "- `csv` – to write data to CSV files.\n",
    "- `re` – for regular expression operations.\n",
    "- `os` – to check if files already exist.\n",
    "- `BeautifulSoup` from `bs4` – to parse HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642b36d",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants\n",
    "Define the base URL, HTTP headers, CSS selectors, and other constants used throughout the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://immovlan.be/en/real-estate?transactiontypes=for-sale,for-rent&propertytypes=apartment,house&propertysubtypes=apartment,ground-floor,duplex,penthouse,studio,loft,triplex,residence,villa,mixed-building,master-house,cottage,bungalow,chalet,mansion&sortdirection=ascending&sortby=price&noindex=1\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "# URL parameters\n",
    "PRICE_FROM_PARAM = \"minprice\"\n",
    "PRICE_TO_PARAM   = \"maxprice\"\n",
    "SORT_PARAM = \"sortby\"\n",
    "SORT_DIRECTION_PARAM = \"sortdirection\"\n",
    "SORT_VALUE = \"price\"\n",
    "SORT_DIRECTION_VALUE = \"ascending\"\n",
    "\n",
    "# CSS selectors\n",
    "CARD_SELECTOR        = \".list-view-item\"          # each listing card on search page\n",
    "LINK_ATTR            = \"data-url\"                  # attribute containing the detail page URL\n",
    "PRICE_SELECTOR       = \".list-item-price\"          # element showing the price on the card\n",
    "PROJECT_EXCLUDE      = \"/projectdetail/\"           # filter out project pages (not individual properties)\n",
    "\n",
    "# CSV file names\n",
    "LINKS_CSV = \"src/property_links.csv\"\n",
    "DETAILS_CSV = \"src/property_details.csv\"\n",
    "\n",
    "# Fields for the details CSV\n",
    "DETAILS_FIELDS = [\n",
    "    \"Link\", \"Locality\", \"Type of property\", \"Subtype of property\",\n",
    "    \"Price\", \"Type of sale\", \"Number of rooms\", \"Livable surface\",\n",
    "    \"Fully equipped kitchen\", \"Furnished\", \"Fireplace\", \"Terrace\",\n",
    "    \"Garden\", \"Total land surface\", \"Number of facades\", \"Swimming pool\",\n",
    "    \"State of the property\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308371d5",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "### 3.1 `fetch_batch(min_price, max_pages=50)`\n",
    "\n",
    "Fetches one batch of search result pages starting from a given minimum price.  \n",
    "It returns:\n",
    "- A list of property links found in this batch.\n",
    "- The last price encountered in the batch (used as the starting point for the next batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(min_price, max_pages=50):\n",
    "    all_links = []\n",
    "    last_price = None\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        if page > max_pages:\n",
    "            break\n",
    "\n",
    "        # Build URL with current min_price and page\n",
    "        url = f\"{base_url}&{PRICE_FROM_PARAM}={min_price}&{SORT_PARAM}={SORT_VALUE}&{SORT_DIRECTION_PARAM}={SORT_DIRECTION_VALUE}\"\n",
    "        if page > 1:\n",
    "            url += f\"&page={page}\"\n",
    "\n",
    "        try:\n",
    "            resp = session.get(url)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        cards = soup.select(CARD_SELECTOR)\n",
    "\n",
    "        if not cards:\n",
    "            break\n",
    "\n",
    "        page_links = []\n",
    "        for card in cards:\n",
    "            link = card.get(LINK_ATTR)\n",
    "            if link and PROJECT_EXCLUDE not in link:\n",
    "                price_elem = card.select_one(PRICE_SELECTOR)\n",
    "                if price_elem:\n",
    "                    price_text = price_elem.get_text(strip=True)\n",
    "                    digits = re.sub(r'[^\\d]', '', price_text)\n",
    "                    if digits:\n",
    "                        price = int(digits)\n",
    "                        last_price = price\n",
    "                if link not in page_links:       \n",
    "                    page_links.append(link)\n",
    "\n",
    "        all_links.extend(page_links)\n",
    "\n",
    "        # Look for the \"Next\" button (two common patterns)\n",
    "        next_btn = (soup.find(\"a\", string=re.compile(r\"Next\", re.I)) or\n",
    "                    soup.find(\"a\", attrs={\"rel\": \"next\"}))\n",
    "        if not next_btn:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return all_links, last_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9fdbb5",
   "metadata": {},
   "source": [
    "### 3.2 `fetch_all_links_dynamic()`\n",
    "\n",
    "Repeatedly calls `fetch_batch`, each time using the last price from the previous batch as the new minimum.  \n",
    "Stops when a batch returns no links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeac2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_links_dynamic():\n",
    "    \"\"\"\n",
    "    Repeatedly fetch batches using the last price from the previous batch as the new minimum.\n",
    "    Continues until a batch returns no links.\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    min_price = 0\n",
    "    batch_num = 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"\\n=== Batch {batch_num}: min_price = {min_price} ===\")\n",
    "        links, last_price = fetch_batch(min_price)\n",
    "\n",
    "        if not links:\n",
    "            print(\"No links found in this batch. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_links.extend(links)\n",
    "        print(f\"Batch {batch_num} collected {len(links)} links. Total so far: {len(all_links)}\")\n",
    "\n",
    "        if last_price is None:\n",
    "            print(\"No price information extracted – cannot determine next min_price. Stopping.\")\n",
    "            break\n",
    "\n",
    "        min_price = last_price + 1\n",
    "        batch_num += 1\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48d93d",
   "metadata": {},
   "source": [
    "### 3.3 `scrape_details_and_store(links, filename=DETAILS_CSV)`\n",
    "\n",
    "Visits each property link, extracts the required fields, and appends them to the details CSV file.  \n",
    "If the file doesn’t exist, it writes a header first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_details_and_store(links, filename=DETAILS_CSV):\n",
    "    \"\"\"\n",
    "    Visit each property link, extract desired fields, and append to details CSV.\n",
    "    \"\"\"\n",
    "    if not links:\n",
    "        print(\"No links to scrape details for.\")\n",
    "        return\n",
    "\n",
    "    file_exists = os.path.exists(filename)\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=DETAILS_FIELDS)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for index, link in enumerate(links, start=1):\n",
    "            print(f\"{index}. Fetching details of {link}\")\n",
    "            try:\n",
    "                resp = session.get(link, timeout=10)\n",
    "                resp.raise_for_status()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch {link}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            detail = {field: \"N/A\" for field in DETAILS_FIELDS}\n",
    "            detail[\"Link\"] = link\n",
    "\n",
    "            # Locality (postal code)\n",
    "            locality_elem = soup.find(\"span\", class_=\"city-line\")\n",
    "            if locality_elem:\n",
    "                text = locality_elem.get_text(strip=True)\n",
    "                match = re.search(r'\\d+', text)\n",
    "                detail[\"Locality\"] = match.group() if match else \"N/A\"\n",
    "\n",
    "            # Subtype & Type of property (based on the title)\n",
    "            title_elem = soup.find(\"span\", class_=\"detail__header_title_main\")\n",
    "            if title_elem:\n",
    "                full_text = title_elem.get_text(strip=True)\n",
    "                normalized = full_text.lower().strip()\n",
    "\n",
    "                # Map property subtype to our internal codes and property type (1=apartment, 2=house)\n",
    "                subtype_mappings = [\n",
    "                    (\"apartment\", \"01\", \"1\"),\n",
    "                    (\"penthous\", \"02\", \"1\"),\n",
    "                    (\"ground floor\", \"03\", \"1\"),\n",
    "                    (\"duplex\", \"04\", \"1\"),\n",
    "                    (\"studio\", \"05\", \"1\"),\n",
    "                    (\"loft\", \"06\", \"1\"),\n",
    "                    (\"triplex\", \"07\", \"1\"),\n",
    "                    (\"residence\", \"11\", \"2\"),\n",
    "                    (\"villa\", \"12\", \"2\"),\n",
    "                    (\"mixed building\", \"13\", \"2\"),\n",
    "                    (\"master house\", \"14\", \"2\"),\n",
    "                    (\"cottage\", \"15\", \"2\"),\n",
    "                    (\"bungalow\", \"16\", \"2\"),\n",
    "                    (\"chalet\", \"17\", \"2\"),\n",
    "                    (\"mansion\", \"18\", \"2\")\n",
    "                ]\n",
    "\n",
    "                # Sort by length (longest first) to avoid partial matches\n",
    "                subtype_mappings.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "                found = False\n",
    "                for pattern, code, prop_type in subtype_mappings:\n",
    "                    if normalized.startswith(pattern):\n",
    "                        detail[\"Type of property\"] = int(prop_type)\n",
    "                        detail[\"Subtype of property\"] = code\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if not found:\n",
    "                    detail[\"Subtype of property\"] = \"N/A\"\n",
    "                    detail[\"Type of property\"] = \"N/A\"\n",
    "            else:\n",
    "                detail[\"Subtype of property\"] = \"N/A\"\n",
    "                detail[\"Type of property\"] = \"N/A\"\n",
    "\n",
    "            # Price\n",
    "            price_elem = soup.find(\"span\", class_=\"detail__header_price_data\")\n",
    "            if price_elem:\n",
    "                price_text = price_elem.get_text(strip=True)\n",
    "                digits = re.sub(r'[^\\d]', '', price_text)\n",
    "                detail[\"Price\"] = int(digits) if digits else \"N/A\"\n",
    "\n",
    "            # Type of sale (1 = rent, 2 = sale)\n",
    "            if title_elem:\n",
    "                full_text = title_elem.get_text(strip=True).lower()\n",
    "                if \"rent\" in full_text:\n",
    "                    detail[\"Type of sale\"] = 1\n",
    "                elif \"sale\" in full_text:\n",
    "                    detail[\"Type of sale\"] = 2\n",
    "                else:\n",
    "                    detail[\"Type of sale\"] = \"N/A\"\n",
    "            else:\n",
    "                detail[\"Type of sale\"] = \"N/A\"\n",
    "\n",
    "            # Helper to extract text from a data row by its <h4> label\n",
    "            def get_data_row(label):\n",
    "                h4 = soup.find(\"h4\", string=re.compile(rf\"^{re.escape(label)}$\", re.I))\n",
    "                if h4:\n",
    "                    p = h4.find_next_sibling(\"p\")\n",
    "                    if p:\n",
    "                        return p.get_text(strip=True)\n",
    "                return None\n",
    "\n",
    "            # Number of bedrooms\n",
    "            bedrooms_text = get_data_row(\"Number of bedrooms\")\n",
    "            if bedrooms_text:\n",
    "                digits = re.sub(r'[^\\d]', '', bedrooms_text)\n",
    "                bedrooms = int(digits) if digits else None\n",
    "            else:\n",
    "                bedrooms = None\n",
    "            detail[\"Number of rooms\"] = bedrooms if bedrooms is not None else \"N/A\"\n",
    "\n",
    "            # Living area\n",
    "            living_text = get_data_row(\"Livable surface\")\n",
    "            if living_text:\n",
    "                digits = re.sub(r'[^\\d]', '', living_text)\n",
    "                living_area = int(digits) if digits else None\n",
    "            else:\n",
    "                living_area = None\n",
    "            detail[\"Livable surface\"] = living_area if living_area is not None else \"N/A\"\n",
    "\n",
    "            # Fully equipped kitchen (0 = No, 1 = Yes)\n",
    "            kitchen_text = get_data_row(\"Kitchen equipment\")\n",
    "            if kitchen_text:\n",
    "                detail[\"Fully equipped kitchen\"] = 0 if kitchen_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Fully equipped kitchen\"] = \"N/A\"\n",
    "\n",
    "            # Furnished\n",
    "            furnished_text = get_data_row(\"Furnished\")\n",
    "            if furnished_text:\n",
    "                detail[\"Furnished\"] = 0 if furnished_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Furnished\"] = \"N/A\"\n",
    "\n",
    "            # Fireplace\n",
    "            fire_text = get_data_row(\"Fireplace\")\n",
    "            if fire_text:\n",
    "                detail[\"Fireplace\"] = 0 if fire_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Fireplace\"] = \"N/A\"\n",
    "\n",
    "            # Terrace\n",
    "            terrace_text = get_data_row(\"Terrace\")\n",
    "            if terrace_text:\n",
    "                detail[\"Terrace\"] = 0 if terrace_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Terrace\"] = \"N/A\"\n",
    "\n",
    "            # Garden\n",
    "            garden_text = get_data_row(\"Garden\")\n",
    "            if garden_text:\n",
    "                detail[\"Garden\"] = 0 if garden_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Garden\"] = \"N/A\"\n",
    "\n",
    "            # Total land surface\n",
    "            land_text = get_data_row(\"Total land surface\")\n",
    "            if land_text:\n",
    "                digits = re.sub(r'[^\\d]', '', land_text)\n",
    "                land_area = int(digits) if digits else None\n",
    "            else:\n",
    "                land_area = None\n",
    "            detail[\"Total land surface\"] = land_area if land_area is not None else \"N/A\"\n",
    "\n",
    "            # Number of facades\n",
    "            facades_text = get_data_row(\"Number of facades\")\n",
    "            if facades_text:\n",
    "                digits = re.sub(r'[^\\d]', '', facades_text)\n",
    "                facades = int(digits) if digits else None\n",
    "            else:\n",
    "                facades = None\n",
    "            detail[\"Number of facades\"] = facades if facades is not None else \"N/A\"\n",
    "\n",
    "            # Swimming pool\n",
    "            pool_text = get_data_row(\"Swimming pool\")\n",
    "            if pool_text:\n",
    "                detail[\"Swimming pool\"] = 0 if pool_text.lower() == \"no\" else 1\n",
    "            else:\n",
    "                detail[\"Swimming pool\"] = \"N/A\"\n",
    "\n",
    "            # State of the property (mapped to codes 1-5)\n",
    "            state_text = \"N/A\"\n",
    "            h4 = soup.find('h4', string=re.compile(r'State of the property', re.I))\n",
    "            if h4:\n",
    "                p = h4.find_next('p')\n",
    "                if p:\n",
    "                    state_text = p.get_text(strip=True)\n",
    "\n",
    "            state_mapping = {\n",
    "                \"new\": 1,\n",
    "                \"excellent\": 2,\n",
    "                \"fully renovated\": 3,\n",
    "                \"normal\": 4,\n",
    "                \"to renovate\": 5,\n",
    "                \"to be renovated\": 5,\n",
    "            }\n",
    "\n",
    "            if state_text and state_text != \"N/A\":\n",
    "                lower_text = state_text.lower().strip()\n",
    "                matched_code = \"N/A\"\n",
    "                for key, code in state_mapping.items():\n",
    "                    if key in lower_text:\n",
    "                        matched_code = code\n",
    "                        break\n",
    "                detail[\"State of the property\"] = matched_code\n",
    "            else:\n",
    "                detail[\"State of the property\"] = \"N/A\"\n",
    "\n",
    "            writer.writerow(detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d7d20",
   "metadata": {},
   "source": [
    "### 4.4 `load_existing_links(filename)` and `store_new_links(new_links, filename)`\n",
    "\n",
    "These are utility functions to manage the links CSV file. They are not used in the main flow but are kept for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_links(filename):\n",
    "    \"\"\"Load existing property URLs from CSV into a set.\"\"\"\n",
    "    existing = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # skip header\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    existing.add(row[0])\n",
    "    return existing\n",
    "\n",
    "def store_new_links(new_links, filename=LINKS_CSV):\n",
    "    \"\"\"Append new links to the CSV file (creates file with header if needed).\"\"\"\n",
    "    if not new_links:\n",
    "        print(\"No new links to store.\")\n",
    "        return\n",
    "    file_exists = os.path.exists(filename)\n",
    "    with open(filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Link\"])\n",
    "        for link in new_links:\n",
    "            writer.writerow([link])\n",
    "    print(f\"Appended {len(new_links)} new links to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d01fa",
   "metadata": {},
   "source": [
    "## 4. Main Execution\n",
    "\n",
    "### 4.1 Collect All Property Links\n",
    "\n",
    "Run the dynamic link collection and save the results to `property_links.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting link collection...\")\n",
    "all_links = fetch_all_links_dynamic()\n",
    "print(f\"\\nTotal links collected: {len(all_links)}\")\n",
    "\n",
    "# Store the links (overwrite any existing file)\n",
    "with open(LINKS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Link\"])\n",
    "    for link in all_links:\n",
    "        writer.writerow([link])\n",
    "print(f\"Links saved to {LINKS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0f60d",
   "metadata": {},
   "source": [
    "### 4.2 Scrape Details for All Collected Links\n",
    "\n",
    "Load the links from the file (or use `all_links` directly) and extract the details.  \n",
    "Each property’s data is appended to `property_details.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting details scraping...\")\n",
    "links_to_scrape = load_existing_links(LINKS_CSV)\n",
    "print(f\"Loaded {len(links_to_scrape)} links from {LINKS_CSV}.\")\n",
    "\n",
    "scrape_details_and_store(list(links_to_scrape), filename=DETAILS_CSV)\n",
    "\n",
    "print(f\"\\nAll done! Details saved to {DETAILS_CSV}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5065c",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "You have now:\n",
    "- Collected all property links from Immovlan using a price‑based batching technique.\n",
    "- Scraped detailed information for each property.\n",
    "- Saved the data into two CSV files: `property_links.csv` and `property_details.csv`.\n",
    "\n",
    "The notebook is modular – you can re‑run the scraping step without re‑collecting links, or modify the extraction logic to suit your needs.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: The site’s structure may change over time, requiring updates to CSS selectors or extraction logic. Always respect the website’s `robots.txt` and terms of service.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
